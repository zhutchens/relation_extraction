{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/pydantic/_internal/_config.py:295: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  warnings.warn(DEPRECATION_MESSAGE, DeprecationWarning)\n",
      "/home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/pydantic/_internal/_config.py:295: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  warnings.warn(DEPRECATION_MESSAGE, DeprecationWarning)\n",
      "/home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/deepeval/__init__.py:53: UserWarning: You are using deepeval version 2.4.1, however version 2.4.2 is available. You should consider upgrading via the \"pip install --upgrade deepeval\" command.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# import ragas.metrics as m\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from src.generator import RAGKGGenerator\n",
    "from src.llms import OpenAIModel, HuggingFaceLLM\n",
    "# from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "# from ragas.llms import LangchainLLMWrapper\n",
    "# from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "\n",
    "load_dotenv()\n",
    "link = os.getenv('cs2')\n",
    "token = os.getenv('OPENAI_API_KEY')\n",
    "os.environ['HF_TOKEN'] = os.getenv('HF_TOKEN') \n",
    "os.environ['OPENAI_API_KEY'] = token\n",
    "\n",
    "cs2_chapters = [\n",
    "    'Data Structures and Algorithms',\n",
    "    'Mathematical Preliminaries',\n",
    "    'Algorithm Analysis',\n",
    "    'Lists, Stacks, and Queues',\n",
    "    'Binary Trees',\n",
    "    'Non-Binary Trees',\n",
    "    'Internal Sorting',\n",
    "    'File Processing and External Sorting',\n",
    "    'Searching',\n",
    "    'Indexing',\n",
    "    'Graphs',\n",
    "    'Lists and Arrays Revisited',\n",
    "    'Advanced Tree Structures',\n",
    "    'Analysis Techniques',\n",
    "    'Lower Bounds',\n",
    "    'Patterns of Algorithms',\n",
    "    'Limits to Computation',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv('data/sorting.csv')\n",
    "# data.columns = ['concept', 'outcome']\n",
    "data = pd.read_csv('data/dsa_clifford_a_shaffer_3_2_java.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_concepts = data['concepts'][3].split('::')\n",
    "actual_concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = RAGKGGenerator(\n",
    "        [cs2_chapters[3]],\n",
    "        300,\n",
    "        100,\n",
    "        OpenAIModel(),\n",
    "        link,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.metrics import SemanticSimilarity, Contextual_F1, AnswerCorrectness\n",
    "from deepeval.metrics import AnswerRelevancyMetric, ContextualPrecisionMetric, ContextualRecallMetric, FaithfulnessMetric, ContextualRelevancyMetric\n",
    "\n",
    "\n",
    "metrics = [AnswerRelevancyMetric(model = extractor.llm), AnswerCorrectness(model = extractor.llm), FaithfulnessMetric(model = extractor.llm), ContextualPrecisionMetric(model = extractor.llm), ContextualRecallMetric(model = extractor.llm), SemanticSimilarity(st_model = extractor.embedding_model)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>this cell takes a while to run so i commented it out for now</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(5):\n",
    "extractor.identify_concepts(5) \n",
    "result = extractor.evaluate('concepts', 5, actual_concepts, metrics)\n",
    "# print('-' * 20)\n",
    "# print(f'OPENAI MODEL: {extractor.llm.get_model_name()}')\n",
    "# print(f'TEMPERATURE: {extractor.llm.temperature}')\n",
    "# print(f'SENTENCE TRANSFORMER: {extractor.embedding_model}')\n",
    "# print(f'TEXTBOOK: dsa_2214')\n",
    "# print(f'CHAPTERS TESTED: {extractor.chapters}')\n",
    "# print('-' * 20)\n",
    "# for i in result:\n",
    "#     print(f\"METRIC: {i['name']} ----> SCORE: {i['score']}\")\n",
    "#     print()\n",
    "#     print(f\"REASON: {i['reason']}\")\n",
    "#     print() \n",
    "#     print(f\"QUERY: {i['input']}\")\n",
    "#     print()\n",
    "#     print(f\"OUTPUT: {i['output']}\")\n",
    "#     print('-' * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_extractor = RAGKGGenerator(\n",
    "    cs2_chapters, \n",
    "    500, \n",
    "    100, \n",
    "    OpenAIModel(),\n",
    "    link\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_extractor.identify_concepts(10) # concepts from cs2 textbook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_extractor.identify_outcomes(10) # outcomes from textbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_extractor.identify_dependencies('concepts') # extracting chapter dependencies using generated concepts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs2_kg = full_extractor.build_knowledge_graph(full_extractor.dependencies, full_extractor.concepts, full_extractor.outcomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_extractor.draw_knowledge_graph(cs2_kg, './visualizations/cs2_kg.html') # saves kg to cs2_kg.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = full_extractor.build_terminology(build_using = [concept for l in full_extractor.concepts for concept in l])\n",
    "# need list[str] to create clusters\n",
    "# creating clusters using concepts, other options include main topics and outcomes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_extractor.visualize_hierarchy([clusters], visual_type = 'cluster map')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_extractor.terminology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.retrieval import RetrievalSystem\n",
    "from src.retrievers import TransformerRetriever, VectorDBRetriever\n",
    "# tr = TransformerRetriever(link, 500, 100, 'msmarco-distilbert-base-tas-b')\n",
    "vdbr = VectorDBRetriever(link, 500, 100, 'msmarco-distilbert-base-tas-b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.pipeline('Lists, Stacks, and Queues', OpenAIModel()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['one of the most important aspects of a course in data structures is that it is where students really learn to program using pointers and dynamic memory al - location, by implementing data structures such as linked lists and trees. it is often where students truly learn recursion. in our curriculum, this is the ﬁrst course where students do signiﬁcant design, because it often requires real data structures to mo - tivate signiﬁcant design exercises. finally, the fundamental differences between memory - based and disk - based data access cannot be appreciated without practical programming experience. for all of these reasons, a data structures course cannot succeed without a signiﬁcant programming component. in our department, the data structures course is one of the most difﬁcult programming course in the curriculum. students should also work problems to develop their analytical abilities. i pro - vide over 450 exercises and suggestions for programming projects. i urge readers to take advantage of them.',\n",
       " 'chapter 13 is intended in part as a source for larger programming exercises. i recommend that all students taking a data structures course be required to im - plement some advanced tree structure, or another dynamic structure of comparable difﬁculty such as the skip list or sparse matrix representations of chapter 12. none of these data structures are signiﬁcantly more difﬁcult to implement than the binary search tree, and any of them should be within a student ’ s ability after completing chapter 5.',\n",
       " 'within an undergraduate program, this textbook is designed for use in either an advanced lower division ( sophomore or junior level ) data structures course, or for a senior level algorithms course. new material has been added in the third edition to support its use in an algorithms course. normally, this text would be used in a course beyond the standard freshman level “ cs2 ” course that often serves as the initial introduction to data structures. readers of this book should typically have two semesters of the equivalent of programming experience, including at least some exposure to java. readers who are already familiar with recursion will have an advantage. students of data structures will also beneﬁt from having ﬁrst completed a good course in discrete mathematics. nonetheless, chapter 2 attempts to give a reasonably complete survey of the prerequisite mathematical topics at the level necessary to understand their use in this book. readers may wish to refer back to the appropriate sections as needed when encountering unfamiliar mathematical material.',\n",
       " 'using the book in class : data structures and algorithms textbooks tend to fall into one of two categories : teaching texts or encyclopedias. books that attempt to do both usually fail at both. this book is intended as a teaching text. i believe it is more important for a practitioner to understand the principles required to select or design the data structure that will best solve some problem than it is to memorize a lot of textbook implementations. hence, i have designed this as a teaching text that covers most standard data structures, but not all. a few data structures that are not widely adopted are included to illustrate important principles. some relatively new data structures that should become widely used in the future are included.',\n",
       " '4. 3. 1 array - based queues 4. 3. 2 linked queues 4. 3. 3 comparison of array - based and linked queues',\n",
       " 'resource constraints on certain key operations, such as search, inserting data records, and deleting data records, normally drive the data structure selection pro - cess. many issues relating to the relative importance of these operations are ad - dressed by the following three questions, which you should ask yourself whenever you must choose a data structure :',\n",
       " '4. 3. 1 array - based queues',\n",
       " '4. 3 queues',\n",
       " '/ * * queue adt * / public interface queue < e > {',\n",
       " '4. 3. 3 comparison of array - based and linked queues',\n",
       " '/ * * array - based queue implementation * / class aqueue < e > implements queue < e > {',\n",
       " 'deque, 141 dequeue, see queue, dequeue design pattern, xiv, 12 – 16, 19',\n",
       " '/ * * linked queue implementation * / class lqueue < e > implements queue < e > {',\n",
       " '/ / pointer to front queue node / / pointer to rear queuenode / / number of elements in queue',\n",
       " 'a far more efﬁcient implementation can be obtained by relaxing the require - ment that all elements of the queue must be in the ﬁrst n positions of the array. we will still require that the queue be stored be in contiguous array positions, but the contents of the queue will be permitted to drift within the array, as illustrated by figure 4. 25. now, both the enqueue and the dequeue operations can be performed in θ ( 1 ) time because no other elements in the queue need be moved.',\n",
       " 'assume that there are n elements in the queue. by analogy to the array - based list implementation, we could require that all elements of the queue be stored in the ﬁrst n positions of the array. if we choose the rear element of the queue to be in position 0, then dequeue operations require only θ ( 1 ) time because the front ele - ment of the queue ( the one being removed ) is the last element in the array. however, enqueue operations will require θ ( n ) time, because the n elements currently in the queue must each be shifted one position in the array. if instead we chose the rear element of the queue to be in position n − 1, then an enqueue operation is equivalent to an append operation on a list. this requires only θ ( 1 ) time. but now, a dequeue operation requires θ ( n ) time, because all of the elements must be shifted down by one position to retain the property that the remaining n − 1 queue elements reside in the ﬁrst n − 1 positions of the array.',\n",
       " 'lists, stacks, and queues',\n",
       " 'an exact - match query is a search for the record whose key value matches a speciﬁed key value. a range query is a search for all records whose key value falls within a speciﬁed range of key values.',\n",
       " 'like the stack, the queue is a list - like structure that provides restricted access to its elements. queue elements may only be inserted at the back ( called an enqueue operation ) and removed from the front ( called a dequeue operation ). queues oper - ate like standing in line at a movie theater ticket counter. 1 if nobody cheats, then newcomers go to the back of the line. the person at the front of the line is the next to be served. thus, queues release their elements in order of arrival. accountants have used queues since long before the existence of computers. they call a queue a “ fifo ” list, which stands for “ first - in, first - out. ” figure 4. 24 shows a sample queue adt. this section presents two implementations for queues : the array - based queue and the linked queue.',\n",
       " 'lists, stacks, and queues',\n",
       " '/ * * put \" it \" in queue * / public void enqueue ( e it ) {',\n",
       " 'emacs text editor, 423, 425 encapsulation, 9 enqueue, see queue, enqueue entry - sequenced ﬁle, 341 enumeration, see traversal equation, representation, 155 equivalence, 25 – 26',\n",
       " '4 lists, stacks, and queues',\n",
       " '4 lists, stacks, and queues',\n",
       " 'chap. 4 lists, stacks, and queues',\n",
       " 'query',\n",
       " 'chap. 4 lists, stacks, and queues',\n",
       " 'figure 11. 15 a queue - based topological sort algorithm.',\n",
       " 'query',\n",
       " 'query',\n",
       " 'figure 4. 27 an array - based queue implementation.',\n",
       " 'an exact - match query is a search for the record whose key value matches a speciﬁed key value. a range query is a search for all records whose key value falls within a speciﬁed range of key values.',\n",
       " 'n elements in the queue if there are n array positions. this means that there are n + 1 different states for the queue ( 0 through n elements are possible ).',\n",
       " '4. 3. 1 array - based queues',\n",
       " 'figure 4. 28 linked queue class implementation.',\n",
       " 'lists, stacks, and queues',\n",
       " 'query',\n",
       " 'range query, 7 – 8, 10, 301, 314,',\n",
       " 'figure 4. 27 an array - based queue implementation.',\n",
       " 'range query, 7 – 8, 10, 301, 314,',\n",
       " 'figure 4. 27 shows an array - based queue implementation. listarray holds the queue elements, and as usual, the queue constructor allows an optional param - eter to set the maximum size of the queue. the array as created is actually large enough to hold one element more than the queue will allow, so that empty queues can be distinguished from full queues. member maxsize is used to control the circular motion of the queue ( it is the base for the modulus operator ). member rear is set to the position of the current rear element, while front is the position of the current front element.',\n",
       " 'figure 4. 28 linked queue class implementation.',\n",
       " '1 4',\n",
       " 'figure 4. 24 the java adt for a queue.',\n",
       " 'figure 11. 15 a queue - based topological sort algorithm.',\n",
       " '4. 3. 1 array - based queues',\n",
       " '4 lists, stacks, and queues',\n",
       " '/ * * queue adt * / public interface queue < e > {',\n",
       " 'exact - match query, see search,',\n",
       " '4',\n",
       " '/ * * array - based queue implementation * / class aqueue < e > implements queue < e > {',\n",
       " '4. 3. 1 array - based queues 4. 3. 2 linked queues 4. 3. 3 comparison of array - based and linked queues',\n",
       " '3',\n",
       " 'figure 4. 28 linked queue class implementation.',\n",
       " 'the array - based queue is somewhat tricky to implement effectively. a simple con - version of the array - based list implementation is not efﬁcient.',\n",
       " '/ / n > 2',\n",
       " '1. large sets of records that are frequently updated. 2. search is by one or a combination of several keys. 3. key range queries or min / max queries are used.',\n",
       " '15. 2. 2 searching in sorted lists',\n",
       " 'emacs text editor, 423, 425 encapsulation, 9 enqueue, see queue, enqueue entry - sequenced ﬁle, 341 enumeration, see traversal equation, representation, 155 equivalence, 25 – 26',\n",
       " 'lists, stacks, and queues',\n",
       " '2',\n",
       " '3. 1',\n",
       " '( 2. 2 )',\n",
       " '4 lists, stacks, and queues',\n",
       " '4. 3. 1 array - based queues 4. 3. 2 linked queues 4. 3. 3 comparison of array - based and linked queues',\n",
       " 'figure 11. 22 an implementation of prim ’ s algorithm using a priority queue.',\n",
       " '/ * * queue adt * / public interface queue < e > {',\n",
       " 'figure 4. 24 the java adt for a queue.',\n",
       " 'assume that there are n elements in the queue. by analogy to the array - based list implementation, we could require that all elements of the queue be stored in the ﬁrst n positions of the array. if we choose the rear element of the queue to be in position 0, then dequeue operations require only θ ( 1 ) time because the front ele - ment of the queue ( the one being removed ) is the last element in the array. however, enqueue operations will require θ ( n ) time, because the n elements currently in the queue must each be shifted one position in the array. if instead we chose the rear element of the queue to be in position n − 1, then an enqueue operation is equivalent to an append operation on a list. this requires only θ ( 1 ) time. but now, a dequeue operation requires θ ( n ) time, because all of the elements must be shifted down by one position to retain the property that the remaining n − 1 queue elements reside in the ﬁrst n − 1 positions of the array.',\n",
       " '( 2. 3 )',\n",
       " '3. 4',\n",
       " 'all member functions for both the array - based and linked queue implementations require constant time. the space comparison issues are the same as for the equiva - lent stack implementations. unlike the array - based stack implementation, there is no convenient way to store two queues in the same array, unless items are always transferred directly from one queue to the other.',\n",
       " 'a far more efﬁcient implementation can be obtained by relaxing the require - ment that all elements of the queue must be in the ﬁrst n positions of the array. we will still require that the queue be stored be in contiguous array positions, but the contents of the queue will be permitted to drift within the array, as illustrated by figure 4. 25. now, both the enqueue and the dequeue operations can be performed in θ ( 1 ) time because no other elements in the queue need be moved.',\n",
       " '15. 2. 1 searching in unsorted lists 15. 2. 2 searching in sorted lists',\n",
       " '( i − 2,',\n",
       " 'searching',\n",
       " '15. 2. 2 searching in sorted lists',\n",
       " '15. 2. 2 searching in sorted lists',\n",
       " '1. the most obvious way to keep a list ordered by frequency would be to store a count of accesses to each record and always maintain records in this or - der. this method will be referred to as count. count is similar to the least frequently used buffer replacement strategy. whenever a record is accessed, it might move toward the front of the list if its number of accesses becomes greater than a record preceding it. thus, count will store the records in the order of frequency that has actually occurred so far. besides requiring space for the access counts, count does not react well to changing frequency of access over time. once a record has been accessed a large number of times under the frequency count system, it will remain near the front of the list regardless of further access history.',\n",
       " 'how efﬁcient is hashing? we can measure hashing performance in terms of the number of record accesses required when performing an operation. the primary operations of concern are insertion, deletion, and search. it is useful to distinguish between successful and unsuccessful searches. before a record can be deleted, it must be found. thus, the number of accesses required to delete a record is equiv - alent to the number required to successfully search for it. to insert a record, an empty slot along the record ’ s probe sequence must be found. this is equivalent to an unsuccessful search for the record ( recall that a successful search for the record during insertion should generate an error because two records with the same key are not allowed to be stored in the table ).',\n",
       " '} else {',\n",
       " 'being conversant with standard proof techniques can help you in this process. knowing how to write a good proof helps in many ways. first, it clariﬁes your thought process, which in turn clariﬁes your explanations. second, if you use one of the standard proof structures such as proof by contradiction or an induction proof, then both you and your reader are working from a shared understanding of that structure. that makes for less complexity to your reader to understand your proof, because the reader need not decode the structure of your argument from scratch.',\n",
       " 'efﬁcient sequential access relies on seek time being kept to a minimum. the ﬁrst requirement is that the blocks making up a ﬁle are in fact stored on disk in sequential order and close together, preferably ﬁlling a small number of contiguous tracks. at the very least, the number of extents making up the ﬁle should be small. users typically do not have much control over the layout of their ﬁle on disk, but writing a ﬁle all at once in sequential order to a disk drive with a high percentage of free space increases the likelihood of such an arrangement.',\n",
       " 'at this point, we have reached the base case for fact, and so the recursion begins to unwind. each return from fact involves popping the stored value for n from the stack, along with the return address from the function call. the return value for fact is multiplied by the restored value for n, and the result is returned. because an activation record must be created and placed onto the stack for each subroutine call, making subroutine calls is a relatively expensive operation. while recursion is often used to make implementation easy and clear, sometimes',\n",
       " 'there are many approaches to solving recurrence relations, and we brieﬂy con - sider three here. the ﬁrst is an estimation technique : guess the upper and lower bounds for the recurrence, use induction to prove the bounds, and tighten as re - quired. the second approach is to expand the recurrence to convert it to a summa - tion and then use summation techniques. the third approach is to take advantage of already proven theorems when the recurrence is of a suitable form. in particu - lar, typical divide and conquer algorithms such as mergesort yield recurrences of a form that ﬁts a pattern for which we have a ready solution.',\n",
       " 'a reasonable database system must answer queries quickly enough to satisfy the patience of a typical user. for an exact - match query, a few sec - onds is satisfactory. if the database is meant to support range queries that can return many cities that match the query speciﬁcation, the entire opera -',\n",
       " 'when data are static, a linear index provides an extremely efﬁcient way to search. the problem is how to handle those pesky inserts and deletes. we could try to keep the core idea of storing a sorted array - based list, but make it more ﬂexible by breaking the list into manageable chunks that are more easily updated. how might we do that? first, we need to decide how big the chunks should be. since the data are on disk, it seems reasonable to store a chunk that is the size of a disk block, or a small multiple of the disk block size. if the next record to be inserted belongs to a chunk that hasn ’ t ﬁlled its block then we can just insert it there. the fact that this might cause other records in that chunk to move a little bit in the array is not important, since this does not cause any extra disk accesses so long as we move data within that chunk. but what if the chunk ﬁlls up the entire block that contains it? we could just split it in half. what if we want to delete a record? we could just take the deleted record out of the chunk, but we might not want a lot of near - empty chunks. so we could put adjacent chunks together if they have only a small amount of data between them. or we could shufﬂe data between adjacent chunks that together contain more data. the big problem would be how to ﬁnd the desired chunk when processing a record with a given key. perhaps some sort of tree - like structure could be used to locate the appropriate chunk. these ideas are exactly what motivate the b + - tree. the b + - tree is essentially a mechanism for managing a sorted array - based list, where the list is broken into chunks.',\n",
       " 'we would like to pick a hash function that stores the actual records in the col - lection such that each slot in the hash table has equal probability of being ﬁlled. un - fortunately, we normally have no control over the key values of the actual records, so how well any particular hash function does this depends on the distribution of the keys within the allowable key range. in some cases, incoming data are well distributed across their key range. for example, if the input is a set of random numbers selected uniformly from the key range, any hash function that assigns the key range so that each slot in the hash table receives an equal share of the range will likely also distribute the input records uniformly within the table. however, in many applications the incoming records are highly clustered or otherwise poorly distributed. when input records are not well distributed throughout the key range it can be difﬁcult to devise a hash function that does a good job of distributing the records throughout the table, especially if the input distribution is not known in advance.',\n",
       " 'quicksort ﬁrst selects a value called the pivot. ( this is conceptually like the root node ’ s value in the bst. ) assume that the input array contains k values less than the pivot. the records are then rearranged in such a way that the k values less than the pivot are placed in the ﬁrst, or leftmost, k positions in the array, and the values greater than or equal to the pivot are placed in the last, or rightmost, n − k positions. this is called a partition of the array. the values placed in a given partition need not ( and typically will not ) be sorted with respect to each other. all that is required is that all values end up in the correct partition. the pivot value itself is placed in position k. quicksort then proceeds to sort the resulting subarrays now on either side of the pivot, one of size k and the other of size n − k − 1. how are these values sorted? because quicksort is such a good algorithm, using quicksort on the subarrays would be appropriate.',\n",
       " 'sequential search, see search, sequential sequential tree implementations,',\n",
       " 'virginia tech helped make this whole thing possible through sabbatical re - search leave during fall 1994, enabling me to get the project off the ground. my de - partment heads during the time i have written the various editions of this book, den - nis kafura and jack carroll, provided unwavering moral support for this project. mike keenan, lenny heath, and jeff shaffer provided valuable input on early ver - sions of the chapters. i also wish to thank lenny heath for many years of stimulat - ing discussions about algorithms and analysis ( and how to teach both to students ). steve edwards deserves special thanks for spending so much time helping me on various redesigns of the c + + and java code versions for the second and third edi - tions, and many hours of discussion on the principles of program design. thanks to layne watson for his help with mathematica, and to bo begole, philip isenhour, jeff nielsen, and craig struble for much technical assistance. thanks to bill mc - quain, mark abrams and dennis kafura for answering lots of silly questions about c + + and java.',\n",
       " 'of m / b slots. the hash function assigns each record to the ﬁrst slot within one of the buckets. if this slot is already occupied, then the bucket slots are searched sequentially until an open slot is found. if a bucket is entirely full, then the record is stored in an overﬂow bucket of inﬁnite capacity at the end of the table. all buckets share the same overﬂow bucket. a good implementation will use a hash function that distributes the records evenly among the buckets so that as few records as possible go into the overﬂow bucket. figure 9. 4 illustrates bucket hashing.',\n",
       " 'consider a large database of employee records. if the primary key is the em - ployee ’ s id number and the secondary key is the employee ’ s name, then each record in the name index associates a name with one or more id numbers. the id number index in turn associates an id number with a unique pointer to the full record on disk. the secondary key index in such an organization is also known as an inverted list or inverted ﬁle. it is inverted in that searches work backwards from the secondary key to the primary key to the actual data record. it is called a list because each secondary key value has ( conceptually ) a list of primary keys as - sociated with it. figure 10. 4 illustrates this arrangement. here, we have last names as the secondary key. the primary key is a four - character unique identiﬁer.',\n",
       " 'when searching for a record, the ﬁrst step is to hash the key to determine which bucket should contain the record. the records in this bucket are then searched. if the desired key value is not found and the bucket still has free slots, then the search is complete. if the bucket is full, then it is possible that the desired record is stored in the overﬂow bucket. in this case, the overﬂow bucket must be searched until the record is found or all records in the overﬂow bucket have been checked. if many records are in the overﬂow bucket, this will be an expensive process.',\n",
       " 'bucket methods are good for implementing hash tables stored on disk, because the bucket size can be set to the size of a disk block. whenever search or insertion occurs, the entire bucket is read into memory. because the entire bucket is then in memory, processing an insert or search operation requires only one disk access, unless the bucket is full. if the bucket is full, then the overﬂow bucket must be retrieved from disk as well. naturally, overﬂow should be kept small to minimize unnecessary disk accesses.',\n",
       " 'sequential search, see search, sequential sequential tree implementations,',\n",
       " 'open hashing is most appropriate when the hash table is kept in main memory, with the lists implemented by a standard in - memory linked list. storing an open hash table on disk in an efﬁcient way is difﬁcult, because members of a given linked list might be stored on different disk blocks. this would result in multiple disk accesses when searching for a particular key value, which defeats the purpose of using hashing.',\n",
       " 'there is no such thing as a random number sequence, only “ random enough ” sequences. a sequence is pseudorandom if no future term can be predicted in polynomial time, given all past terms.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vdbr.pipeline('What are queues?', OpenAIModel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vdbr.semantic_search('What are queues?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop_chapters = [\n",
    "    'Meet Hadoop',\n",
    "    'MapReduce',\n",
    "    'The Hadoop Distributed Filesystem',\n",
    "    'YARN',\n",
    "    'Hadoop I/O',\n",
    "    'Developing a MapReduce Application',\n",
    "    'How MapReduce Works',\n",
    "    'MapReduce Types and Formats',\n",
    "    'MapReduce Features',\n",
    "    'Setting up a Hadoop Cluster',\n",
    "    'Administering Hadoop',\n",
    "    'Avro',\n",
    "    'Parquet',\n",
    "    'Flume',\n",
    "    'Sqoop',\n",
    "    'Pig',\n",
    "    'Hive',\n",
    "    'Crunch',\n",
    "    'Spark',\n",
    "    'HBase',\n",
    "    'Zookeeper',\n",
    "    'Composable Data at Cerner',\n",
    "    'Biological Data Science: Saving Lives with Software.',\n",
    "    'Cascading'\n",
    "]\n",
    "\n",
    "hadoop_extractor = RAGKGGenerator(\n",
    "    hadoop_chapters,\n",
    "    500,\n",
    "    100,\n",
    "    OpenAIModel(),\n",
    "    os.getenv('ccda'),\n",
    "    'data/Syllabus for 202380-Fall 2023-ITCS-3190-001-Cloud Comp for Data Analysis.pdf'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clusters, is_a_relationships, topic_objectives, syllabus_objectives, main_topics = hadoop_extractor.syllabus_pipline()\n",
    "kg = hadoop_extractor.syllabus_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop_extractor.clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop_extractor.draw_knowledge_graph(kg, './visualizations/syllabus_hadoop_kg.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop_extractor.visualize_hierarchy(hadoop_extractor.clusters, 'cluster map')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.metrics import AnswerCorrectness\n",
    "from deepeval.test_case import LLMTestCase\n",
    "cloud_computing_learning_topics = ['Distributed Computing and Clouds', 'Data Analysis Algorithms', 'Hadoop', 'HDFS', 'YARN', 'MapReduce', 'Pig', 'Hive', 'Spark', 'Information Retrieval', 'Page Rank', 'Web Search']\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "ac = AnswerCorrectness()\n",
    "ac.measure(LLMTestCase('No input given.', ' '.join(hadoop_extractor.main_topics), ' '.join(cloud_computing_learning_topics)))\n",
    "\n",
    "print('actual:', hadoop_extractor.main_topics)\n",
    "print('expected:', cloud_computing_learning_topics)\n",
    "print(ac.score)\n",
    "print(ac.reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = hadoop_extractor.identify_main_topics()\n",
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop_extractor.objectives_from_syllabus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations = hadoop_extractor.identify_main_topic_relations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop_extractor.draw_graph(relations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "56211c6593777bedeb9a19153ebc7701344247d055e998aec252a1f471490a08"
  },
  "kernelspec": {
   "display_name": "Python 3.10.12 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
